{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/root/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Author  : nsytsqdtn\n",
    "# @Blog    ：https://www.nsytsqdtn.cn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "import collections\n",
    "import re\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from transformers import BertForPreTraining, BertModel, BertTokenizer\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "from bert4keras.tokenizers import Tokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "from torchcrf import CRF\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import *\n",
    "torch.set_printoptions(edgeitems=768)\n",
    "tqdm.pandas()\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BERT_MODEL_PATH = '../../../pre_model/chinese_roberta_wwm_ext_pytorch/'\n",
    "dict_path = '../../../pre_model/chinese_roberta_wwm_ext_pytorch/vocab.txt'\n",
    "# 设置基本参数\n",
    "MAX_LEN = 20\n",
    "BATCH_SIZE = 8\n",
    "SEP_TOKEN_ID = 102\n",
    "SEED=2019\n",
    "NAME = 'robert'\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id            question\n",
      "0         0            采荷一小是分校吧\n",
      "1         1                毛坯吗？\n",
      "2         2  你们的佣金费大约是多少和契税是多少。\n",
      "3         3             靠近川沙路嘛？\n",
      "4         4      这套房源价格还有优惠空间吗？\n",
      "   query_id  reply_id                       answer  label\n",
      "0         0         0  杭州市采荷第一小学钱江苑校区，杭州市钱江新城实验学校。      1\n",
      "1         0         1                           是的      0\n",
      "2         0         2                         这是5楼      0\n",
      "3         1         0                   因为公积金贷款贷的少      0\n",
      "4         1         1                           是呢      0\n"
     ]
    }
   ],
   "source": [
    "train_query = pd.read_csv('../../data/house/train/train.query.tsv', sep='\\t', header=None)\n",
    "train_query.rename(columns={0:'query_id', 1:'question'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "train_reply = pd.read_csv('../../data/house/train/train.reply.tsv', sep='\\t', header=None)\n",
    "train_reply.rename(columns={0:'query_id', 1:'reply_id', 2:'answer', 3:'label'}, inplace=True)\n",
    "train_data = train_query.merge(train_reply, on='query_id', how='left')\n",
    "\n",
    "train_reply['answer'] = train_reply['answer'].astype('str')\n",
    "\n",
    "test_query = pd.read_csv('../../data/house/test/test.query.tsv', sep='\\t', header=None, encoding='GB18030')\n",
    "test_query.rename(columns={0:'query_id', 1:'question'}, inplace=True)\n",
    "\n",
    "test_reply = pd.read_csv('../../data/house/test/test.reply.tsv', sep='\\t', header=None, encoding='GB18030')\n",
    "test_reply.rename(columns={0:'query_id', 1:'reply_id', 2:'answer'}, inplace=True)\n",
    "test_reply['answer'] = test_reply['answer'].astype('str')\n",
    "print(train_query.head())\n",
    "print(train_reply.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd6ed4294874c4a959e6a6cdce8f576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = []\n",
    "for i in tqdm_notebook(range(len(train_query))):\n",
    "    train_dict = {}\n",
    "    train_dict['query_id'] = train_query.loc[i, 'query_id']\n",
    "    train_dict['question'] = train_query.loc[i, 'question']\n",
    "    train_qa = train_reply[train_reply['query_id'] == i].reset_index()\n",
    "    reply_li = []\n",
    "    for j in range(len(train_qa)):\n",
    "        reply_dict = {}\n",
    "        reply_id = train_qa.loc[j, 'reply_id']\n",
    "        answer = train_qa.loc[j, 'answer']\n",
    "        label = train_qa.loc[j, 'label']\n",
    "        reply_dict['reply_id'] = reply_id\n",
    "        reply_dict['reply'] = answer\n",
    "        reply_dict['label'] = label\n",
    "        reply_li.append(reply_dict)\n",
    "    train_dict['all_reply'] = reply_li\n",
    "    train_dataset.append(train_dict)\n",
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c457608228b046129587ae7b68f552d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query_id': 0,\n",
       " 'question': '东区西区？什么时候下证？',\n",
       " 'all_reply': [{'reply_id': 0, 'reply': '我在给你发套'},\n",
       "  {'reply_id': 1, 'reply': '您看下我发的这几套'},\n",
       "  {'reply_id': 2, 'reply': '这两套也是金源花园的'},\n",
       "  {'reply_id': 3, 'reply': '价钱低'},\n",
       "  {'reply_id': 4, 'reply': '便宜的房子，一般都是顶楼'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = []\n",
    "for i in tqdm_notebook(range(len(test_query))):\n",
    "    test_dict = {}\n",
    "    test_dict['query_id'] = test_query.loc[i, 'query_id']\n",
    "    test_dict['question'] = test_query.loc[i, 'question']\n",
    "    test_qa = test_reply[test_reply['query_id'] == i].reset_index()\n",
    "    reply_li = []\n",
    "    for j in range(len(test_qa)):\n",
    "        reply_dict = {}\n",
    "        reply_id = test_qa.loc[j, 'reply_id']\n",
    "        answer = test_qa.loc[j, 'answer']\n",
    "        reply_dict['reply_id'] = reply_id\n",
    "        reply_dict['reply'] = answer\n",
    "        reply_li.append(reply_dict)\n",
    "    test_dict['all_reply'] = reply_li\n",
    "    test_dataset.append(test_dict)\n",
    "test_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(data.Dataset):\n",
    "    def __init__(self, data, mode='train'):\n",
    "        self.data = data\n",
    "        self.tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
    "        self.mode = mode\n",
    "        self.dataset = self.get_data(self.data, self.tokenizer,self.mode)\n",
    "        \n",
    "    def get_data(self, data, tokenizer, mode):\n",
    "        dataset = []\n",
    "        for data_li in tqdm_notebook(data):\n",
    "            query_id = data_li['query_id']\n",
    "            question = data_li['question']\n",
    "            all_reply = data_li['all_reply']\n",
    "            question_tokens = tokenizer.tokenize(question)\n",
    "            for reply_li in all_reply:\n",
    "                reply_id = reply_li['reply_id']\n",
    "                reply = reply_li['reply']\n",
    "                reply_tokens = tokenizer.tokenize(reply)[1:]\n",
    "                qa_token = (question_tokens + reply_tokens)\n",
    "                qa_token = qa_token[:MAX_LEN]\n",
    "                qa_token_ids = tokenizer.tokens_to_ids(qa_token)\n",
    "                if len(qa_token_ids) < MAX_LEN:\n",
    "                    qa_token_ids += [0] * (MAX_LEN-len(qa_token_ids))\n",
    "                labels = None\n",
    "                if mode != 'test':\n",
    "                    labels = reply_li['label']\n",
    "                dataset_dict = {'query_id':query_id, 'question':question, 'reply_id':reply_id, 'reply':reply,\n",
    "                            'token_ids':qa_token_ids, 'labels': labels}\n",
    "                dataset.append(dataset_dict)\n",
    "        return dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        token_ids = torch.tensor(data['token_ids'])\n",
    "        seg_ids = self.get_seg_ids(token_ids)\n",
    "        if self.mode == 'test':\n",
    "            return token_ids, seg_ids\n",
    "        else:\n",
    "            labels = torch.tensor(data['labels'])\n",
    "            return token_ids, seg_ids, labels\n",
    "    \n",
    "    def get_seg_ids(self, ids):\n",
    "        seg_ids = torch.zeros_like(ids)\n",
    "        seg_idx = 0\n",
    "        for i, e in enumerate(ids):\n",
    "            seg_ids[i] = seg_idx\n",
    "            if e == SEP_TOKEN_ID:\n",
    "                seg_idx += 1\n",
    "        max_idx = torch.nonzero(seg_ids == seg_idx)\n",
    "        seg_ids[max_idx] = 0\n",
    "        return seg_ids\n",
    "\n",
    "def get_dataloader(dataset, mode):\n",
    "    torchdata = DataSet(dataset, mode=mode)\n",
    "    if mode == 'train':\n",
    "        dataloader = torch.utils.data.DataLoader(torchdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "    elif mode == 'test':\n",
    "        dataloader = torch.utils.data.DataLoader(torchdata, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False)\n",
    "    elif mode == 'valid':\n",
    "        dataloader = torch.utils.data.DataLoader(torchdata, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False)\n",
    "    return dataloader, torchdata\n",
    "\n",
    "# train_dataloader, train_torchdata = get_dataloader(train_dataset, mode='train')\n",
    "# test_dataloader, test_torchdata = get_dataloader(test_dataset, mode='test')\n",
    "# train_torchdata.get_seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT_Model, self).__init__()\n",
    "        self.hidden_size = 768\n",
    "        self.bert = BertModel.from_pretrained(BERT_MODEL_PATH, output_hidden_states=True)\n",
    "        self.linear = nn.Linear(self.hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_ids, seg_ids, labels_ids=None, mode='test'):\n",
    "        attention_mask = (input_ids > 0)\n",
    "        last_states, pooled_output, hidden_states = self.bert(input_ids=input_ids, token_type_ids=seg_ids, attention_mask=attention_mask)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        seq_relationship_score = self.linear(pooled_output)\n",
    "\n",
    "        if labels_ids is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(seq_relationship_score.view(-1,1).float(), labels_ids.view(-1,1).float())\n",
    "            return loss\n",
    "        else:\n",
    "            return seq_relationship_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_metric(pred_results, true_results):\n",
    "    totol_number=0\n",
    "    predict_number=0\n",
    "    predict_correct=0\n",
    "    precision = 0\n",
    "    recall = 0\n",
    "    f1_score = 0\n",
    "    for i in range(len(pred_results)):\n",
    "        pred = pred_results[i]\n",
    "        true = true_results[i]\n",
    "        if pred == 1:\n",
    "            predict_number += 1\n",
    "        if true == 1:\n",
    "            totol_number += 1\n",
    "        if pred == 1 and true == 1:\n",
    "            predict_correct += 1\n",
    "    print('实际为1个数：{}   预测为1个数：{}   预测正确个数：{}  '.format(totol_number,predict_number,predict_correct))\n",
    "    if predict_number == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = predict_correct/predict_number\n",
    "    if totol_number == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = predict_correct/totol_number\n",
    "    if precision == 0 or recall == 0:\n",
    "        f1_score = 0\n",
    "    else:\n",
    "        f1_score = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def validation_funtion(model, valid_dataloader, valid_torchdata, mode):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    true_label = []\n",
    "    if valid_torchdata.dataset[0]['labels'] != None:\n",
    "        for i, (input_ids, seg_ids, label_ids) in enumerate(tqdm_notebook(valid_dataloader)):\n",
    "            output = model(input_ids.to(DEVICE), seg_ids.to(DEVICE), mode='test')\n",
    "            results += list(output.detach().cpu()) \n",
    "            true_label += list(label_ids)\n",
    "    else:\n",
    "        for i, (input_ids, seg_ids) in enumerate(tqdm_notebook(valid_dataloader)):\n",
    "            output = model(input_ids.to(DEVICE), seg_ids.to(DEVICE), None, mode='test')\n",
    "            results += list(output.detach().cpu())    \n",
    "    key = 0.5\n",
    "    results = [1 if result > key else 0 for result in results]\n",
    "    if mode == 'valid':\n",
    "        precision, recall, f1_score = f1_metric(results, true_label)\n",
    "        return precision, recall, f1_score\n",
    "    else:\n",
    "        return results\n",
    "                            \n",
    "def train(model, train_dataloader, valid_dataloader, valid_torchdata, epochs, early_stop=None,logger=None):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.8},\n",
    "            {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5)\n",
    "    total_loss = []\n",
    "    train_loss = []\n",
    "    best_f1 = -np.inf\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        bar = tqdm_notebook(train_dataloader)\n",
    "        for i, (input_ids, seg_ids, label_ids) in enumerate(bar):\n",
    "            output = model(input_ids.to(DEVICE), seg_ids.to(DEVICE), label_ids.to(DEVICE), mode='train')\n",
    "            loss = output\n",
    "            loss.backward()\n",
    "            train_loss.append(loss.item())            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            bar.set_postfix(tloss=np.array(train_loss).mean())\n",
    "        precision, recall, f1_score = validation_funtion(model, valid_dataloader, valid_torchdata, 'valid')\n",
    "        print('train_loss: {}, precision: {}, recall: {}, f1_score: {}\\n'.format(train_loss[-1], precision, recall, f1_score))\n",
    "        logger.info('Epoch:[{}]\\t precision={:.3f}\\t recall={:.3f}\\t f1_score={:.3f}'.format(epoch, precision, recall, f1_score))\n",
    "        global model_num\n",
    "        if epoch == epochs-2:\n",
    "            torch.save(model.state_dict(), 'model/{}_model_{}.bin'.format(NAME,model_num))\n",
    "            model_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 --------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc59d3625d8b4f959abf8266b379d278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c31844be3844fc48c17915a240ab825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fc6a8b6183c43ed8b21db43615758e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=113.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb2d6f77445479ab1cad631e444f27d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=115.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-06 01:11:04,815][<ipython-input-12-504499fe7324>][line:76][INFO] Epoch:[0]\t precision=0.000\t recall=0.000\t f1_score=0.000\n",
      "[2021-03-06 01:11:04,815][<ipython-input-12-504499fe7324>][line:76][INFO] Epoch:[0]\t precision=0.000\t recall=0.000\t f1_score=0.000\n",
      "[2021-03-06 01:11:04,815][<ipython-input-12-504499fe7324>][line:76][INFO] Epoch:[0]\t precision=0.000\t recall=0.000\t f1_score=0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "实际为1个数：231   预测为1个数：0   预测正确个数：0  \n",
      "train_loss: 0.24565573036670685, precision: 0, recall: 0.0, f1_score: 0\n",
      "\n",
      "2 --------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10746392eddb48e79f1ac7be37735761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4daa980a0274f228c00e95f83acb360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=250.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e705adb6cffd4cce9e668f5c4ed011fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bef2a26204e4792bb1b15def29e77e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-03-06 01:11:33,140][<ipython-input-12-504499fe7324>][line:76][INFO] Epoch:[0]\t precision=0.000\t recall=0.000\t f1_score=0.000\n",
      "[2021-03-06 01:11:33,140][<ipython-input-12-504499fe7324>][line:76][INFO] Epoch:[0]\t precision=0.000\t recall=0.000\t f1_score=0.000\n",
      "[2021-03-06 01:11:33,140][<ipython-input-12-504499fe7324>][line:76][INFO] Epoch:[0]\t precision=0.000\t recall=0.000\t f1_score=0.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "实际为1个数：218   预测为1个数：0   预测正确个数：0  \n",
      "train_loss: 0.23786219954490662, precision: 0, recall: 0.0, f1_score: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FOLD = 5\n",
    "kf = KFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "model_num = 1\n",
    "test_preds_total = collections.defaultdict(list)\n",
    "logger = get_logger('logging/{}.log'.format(NAME))\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_dataset[:500])):\n",
    "    print(str(i+1), '-'*50)\n",
    "    tra = [train_dataset[index] for index in train_index]\n",
    "    val = [train_dataset[index] for index in test_index]\n",
    "    train_dataloader, _ = get_dataloader(tra, mode='train')\n",
    "    valid_dataloader, valid_torchdata = get_dataloader(val, mode='valid')\n",
    "    model = BERT_Model()\n",
    "    model.to(DEVICE)\n",
    "    losses = train(model,train_dataloader,\n",
    "                    valid_dataloader,\n",
    "                    valid_torchdata,\n",
    "                    epochs=10,\n",
    "                    early_stop=2,\n",
    "                    logger=logger)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db50dc91023044cd9756f5876bc04174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=14000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_preds_total = []\n",
    "test_dataloader, test_torchdata = get_dataloader(test_dataset, mode='test')\n",
    "for i in range(1,model_num):\n",
    "    model.load_state_dict(torch.load('model/{}_model_{}.bin'.format(NAME, i)))\n",
    "    test_pred_results, _ = validation_funtion(model, test_dataloader, test_torchdata, 'test')\n",
    "    test_preds_total.append(test_pred_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-38302edcd096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_preds_merge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mSum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_preds_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_preds_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_preds_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_preds_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtest_preds_total\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtest_preds_merge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_preds_merge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_preds_merge = []\n",
    "for x in range(len(test_preds_total[0])):\n",
    "    Sum = test_preds_total[0][x]+test_preds_total[1][x]+test_preds_total[2][x]+test_preds_total[3][x]+test_preds_total[4][x]\n",
    "    test_preds_merge.append(Sum)\n",
    "for x in range(len(test_preds_merge)):\n",
    "    if test_preds_merge[x] > 2:\n",
    "        test_preds_merge[x] = 1\n",
    "    else:\n",
    "        test_preds_merge[x] = 0\n",
    "def result_deal():\n",
    "    result = pd.DataFrame()\n",
    "    query_id = []\n",
    "    reply_id = []\n",
    "    for x in range(len(test_dataset)):\n",
    "        for y in range(len(test_dataset[x]['reply_id'])):\n",
    "            query_id.append(test_dataset[x]['query_id'])\n",
    "            reply_id.append(test_dataset[x]['reply_id'][y])\n",
    "    result = pd.DataFrame({'query_id':query_id, 'reply_id':reply_id, 'labels': test_preds_merge})\n",
    "    result.to_csv('result/{}_predict.tsv'.format(NAME), sep='\\t',encoding='gbk',header=None,index = None)\n",
    "    \n",
    "result_deal()\n",
    "Count = test_preds_merge.count(1)\n",
    "print(Count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
