{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import re\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "import warnings\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import gensim\n",
    "from torchcontrib.optim import SWA\n",
    "import os\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import *\n",
    "torch.set_printoptions(edgeitems=768)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 设置基本参数\n",
    "MAX_LEN = 100\n",
    "BATCH_SIZE = 16\n",
    "SEED = 9797\n",
    "NAME = 'capsuleNet'\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if DEVICE=='cuda':\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>report_ID</th>\n",
       "      <th>description</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>623 328 538 382 399 400 478 842 698 137 492 26...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>48 328 538 382 809 623 434 355 382 382 363 145...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>623 656 293 851 636 842 698 493 338 266 369 69...</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>48 328 380 259 439 107 380 265 172 470 290 693...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>623 328 399 698 493 338 266 14 177 415 511 647...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>290 380 247 263 48 328 697 582 91 400 478 842 ...</td>\n",
       "      <td>0 7 15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>852 611 501 582 177 230 294 39 363 180 519 421...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>852 328 290 380 256 544 636 90 735 374 698 116...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>852 328 305 461 382 697 259 779 59 261 589 693...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>722 623 411 382 570 399 328 380 728 672 846 48...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     report_ID                                        description   label\n",
       "0            0  623 328 538 382 399 400 478 842 698 137 492 26...       2\n",
       "1            1  48 328 538 382 809 623 434 355 382 382 363 145...        \n",
       "2            2  623 656 293 851 636 842 698 493 338 266 369 69...      15\n",
       "3            3  48 328 380 259 439 107 380 265 172 470 290 693...        \n",
       "4            4  623 328 399 698 493 338 266 14 177 415 511 647...      16\n",
       "...        ...                                                ...     ...\n",
       "9995      9995  290 380 247 263 48 328 697 582 91 400 478 842 ...  0 7 15\n",
       "9996      9996  852 611 501 582 177 230 294 39 363 180 519 421...      10\n",
       "9997      9997  852 328 290 380 256 544 636 90 735 374 698 116...        \n",
       "9998      9998  852 328 305 461 382 697 259 779 59 261 589 693...      16\n",
       "9999      9999  722 623 411 382 570 399 328 380 728 672 846 48...      16\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('../data/track1_round1_train_20210222.csv',header=None)\n",
    "test_data = pd.read_csv('../data/track1_round1_testB.csv',header=None) \n",
    "train_data.columns=['report_ID','description','label']\n",
    "test_data.columns=['report_ID','description']\n",
    "\n",
    "temp=[i[:-1] for i in train_data['report_ID'].values]\n",
    "train_data['report_ID']=temp\n",
    "temp=[i[:-1] for i in test_data['report_ID'].values]\n",
    "test_data['report_ID']=temp\n",
    "\n",
    "temp=[i.strip('|').strip() for i in train_data['description'].values]\n",
    "train_data['description']=temp\n",
    "temp=[i.strip('|').strip() for i in test_data['description'].values]\n",
    "test_data['description']=temp\n",
    "\n",
    "temp_label=[i.strip('|').strip() for i in train_data['label'].values]\n",
    "train_data['label']=temp_label\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = pd.concat([train_data['description'],test_data['description']]).reset_index(drop=True)\n",
    "all_sentences.drop_duplicates().reset_index(drop=True, inplace=True)\n",
    "all_sentences = all_sentences.apply(lambda x:x.split(' ')).tolist()\n",
    "if not os.path.exists('../embedding/w2v.model'): \n",
    "    w2v_model = gensim.models.word2vec.Word2Vec(all_sentences, sg=1, size=300,window=7,min_count=1,negative=3,sample=0.001,hs=1,seed=452)\n",
    "    w2v_model.save('../embedding/w2v.model')\n",
    "else:\n",
    "    w2v_model = gensim.models.word2vec.Word2Vec.load(\"../embedding/w2v.model\")\n",
    "    \n",
    "if not os.path.exists('../embedding/fasttext.model'): \n",
    "    fasttext_model = gensim.models.FastText(all_sentences, seed=452, size=100, min_count=1, iter=20, window=2)\n",
    "    fasttext_model.save('../embedding/fasttext.model')\n",
    "else:\n",
    "    fasttext_model = gensim.models.word2vec.Word2Vec.load(\"../embedding/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cd63679c60c4eaf833ecebbf6019c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efddd73d00104c14b11d69309d7deffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset = []\n",
    "for i in tqdm(range(len(train_data))):\n",
    "    train_dict = {}\n",
    "    train_dict['report_ID'] = train_data.loc[i, 'report_ID']\n",
    "    train_dict['description'] = train_data.loc[i, 'description']\n",
    "    train_dict['label'] = train_data.loc[i, 'label']\n",
    "    train_dataset.append(train_dict)\n",
    "test_dataset = []\n",
    "for i in tqdm(range(len(test_data))):\n",
    "    test_dict = {}\n",
    "    test_dict['report_ID'] = test_data.loc[i, 'report_ID']\n",
    "    test_dict['description'] = test_data.loc[i, 'description']\n",
    "    test_dict['label'] = ''\n",
    "    test_dataset.append(test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592bed91454d4e5dab9f3652f2d3ac9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566b32c1e88e46948b168e8ca93a959a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class DataSet(data.Dataset):\n",
    "    def __init__(self, data, mode='train'):\n",
    "        self.data = data\n",
    "        self.mode = mode\n",
    "        self.dataset = self.get_data(self.data,self.mode)\n",
    "        \n",
    "    def get_data(self, data, mode):\n",
    "        dataset = []\n",
    "        global s\n",
    "        for data_li in tqdm(data):\n",
    "            description = data_li['description'].split(' ')\n",
    "            description = [w2v_model.wv.vocab[s].index+1 if s in w2v_model.wv.vocab else 0 for s in description]\n",
    "            if len(description) < MAX_LEN:\n",
    "                description += [0] * (MAX_LEN - len(description))\n",
    "            else:\n",
    "                description = description[:MAX_LEN]\n",
    "            label = self.get_dumm(data_li['label'])\n",
    "            dataset_dict = {'description':description, 'label':label}\n",
    "            dataset.append(dataset_dict)\n",
    "        return dataset\n",
    "    \n",
    "    def get_dumm(self,s):\n",
    "        re = [0] * 17\n",
    "        if s == '':\n",
    "            return re\n",
    "        else:\n",
    "            tmp = [int(i) for i in s.split(' ')]\n",
    "            for i in tmp:\n",
    "                re[i] = 1\n",
    "        return re\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.dataset[idx]\n",
    "        description = torch.tensor(data['description'])\n",
    "        if self.mode == 'test':\n",
    "            return description\n",
    "        else:\n",
    "            label = torch.tensor(data['label'])\n",
    "            return description, label\n",
    "\n",
    "def get_dataloader(dataset, mode):\n",
    "    torchdata = DataSet(dataset, mode=mode)\n",
    "    if mode == 'train':\n",
    "        dataloader = torch.utils.data.DataLoader(torchdata, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, drop_last=True)\n",
    "    elif mode == 'test':\n",
    "        dataloader = torch.utils.data.DataLoader(torchdata, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False)\n",
    "    elif mode == 'valid':\n",
    "        dataloader = torch.utils.data.DataLoader(torchdata, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, drop_last=False)\n",
    "    return dataloader, torchdata\n",
    "\n",
    "train_dataloader, train_torchdata = get_dataloader(train_dataset, mode='train')\n",
    "test_dataloader, test_torchdata = get_dataloader(test_dataset, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CyclicLR(object):\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_capsule = 5\n",
    "Dim_capsule = 5\n",
    "class Caps_Layer(nn.Module):\n",
    "    def __init__(self, input_dim_capsule, num_capsule=Num_capsule, dim_capsule=Dim_capsule, \\\n",
    "                 routings=4, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Caps_Layer, self).__init__(**kwargs)\n",
    "        self.T_epsilon = 1e-7\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = 4\n",
    "        self.kernel_size = kernel_size  # 暂时没用到\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = self.squash\n",
    "        else:\n",
    "            self.activation = nn.ReLU(inplace=True)\n",
    "\n",
    "        if self.share_weights:\n",
    "            self.W = nn.Parameter(\n",
    "                nn.init.xavier_normal_(torch.empty(1, input_dim_capsule, self.num_capsule * self.dim_capsule)))\n",
    "        else:\n",
    "            self.W = nn.Parameter(\n",
    "                torch.randn(BATCH_SIZE, input_dim_capsule, self.num_capsule * self.dim_capsule))  # 64即batch_size\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = torch.matmul(x, self.W)\n",
    "        else:\n",
    "            print('add later')\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        input_num_capsule = x.size(1)\n",
    "        u_hat_vecs = u_hat_vecs.view((batch_size, input_num_capsule,\n",
    "                                      self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = u_hat_vecs.permute(0, 2, 1, 3)  # 转成(batch_size,num_capsule,input_num_capsule,dim_capsule)\n",
    "        b = torch.zeros_like(u_hat_vecs[:, :, :, 0])  # (batch_size,num_capsule,input_num_capsule)\n",
    "\n",
    "        for i in range(self.routings):\n",
    "            b = b.permute(0, 2, 1)\n",
    "            c = F.softmax(b, dim=2)\n",
    "            c = c.permute(0, 2, 1)\n",
    "            b = b.permute(0, 2, 1)\n",
    "            outputs = self.activation(torch.einsum('bij,bijk->bik', (c, u_hat_vecs)))  # batch matrix multiplication\n",
    "            # outputs shape (batch_size, num_capsule, dim_capsule)\n",
    "            if i < self.routings - 1:\n",
    "                b = torch.einsum('bik,bijk->bij', (outputs, u_hat_vecs))  # batch matrix multiplication\n",
    "        return outputs  # (batch_size, num_capsule, dim_capsule)\n",
    "\n",
    "    # text version of squash, slight different from original one\n",
    "    def squash(self, x, axis=-1):\n",
    "        s_squared_norm = (x ** 2).sum(axis, keepdim=True)\n",
    "        scale = torch.sqrt(s_squared_norm + self.T_epsilon)\n",
    "        return x / scale\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self,vocab_size,embedding_dim,embeddings=None):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.num_classes = 17\n",
    "        fc_layer = 256\n",
    "        hidden_size = 128\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        if embeddings:\n",
    "            w2v_model = gensim.models.word2vec.Word2Vec.load(\"../embedding/w2v.model\").wv\n",
    "            fasttext_model = gensim.models.word2vec.Word2Vec.load(\"../embedding/fasttext.model\").wv\n",
    "            w2v_embed_matrix = w2v_model.vectors\n",
    "            fasttext_embed_matrix = fasttext_model.vectors\n",
    "#             embed_matrix = w2v_embed_matrix         \n",
    "            embed_matrix = np.concatenate([w2v_embed_matrix, fasttext_embed_matrix], axis=1)\n",
    "            oov_embed = np.zeros((1, embed_matrix.shape[1]))\n",
    "            embed_matrix = torch.from_numpy(np.vstack((oov_embed,embed_matrix)))\n",
    "            self.embedding.weight.data.copy_(embed_matrix)\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.GRU(embedding_dim, hidden_size,2, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size,2, bidirectional=True, batch_first=True)\n",
    "        self.tdbn = nn.BatchNorm2d(1)\n",
    "        self.lstm_attention = Attention(hidden_size * 2, MAX_LEN)\n",
    "        self.gru_attention = Attention(hidden_size * 2, MAX_LEN)\n",
    "        self.bn = nn.BatchNorm1d(fc_layer)\n",
    "        self.linear = nn.Linear(hidden_size*8+1, fc_layer)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.output = nn.Linear(fc_layer, self.num_classes)\n",
    "        self.lincaps = nn.Linear(Num_capsule * Dim_capsule, 1)\n",
    "        self.caps_layer = Caps_Layer(hidden_size*2)\n",
    "    def forward(self, x, label=None):\n",
    "        \n",
    "#         Capsule(num_capsule=10, dim_capsule=10, routings=4, share_weights=True)(x)\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(\n",
    "            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        h_embedding = self.tdbn(h_embedding.unsqueeze(1)).squeeze(1)\n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "\n",
    "        ##Capsule Layer        \n",
    "        content3 = self.caps_layer(h_gru)\n",
    "        content3 = self.dropout(content3)\n",
    "        batch_size = content3.size(0)\n",
    "        content3 = content3.view(batch_size, -1)\n",
    "        content3 = self.relu(self.lincaps(content3))\n",
    "\n",
    "        ##Attention Layer\n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten,content3, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.bn(conc)\n",
    "        out = self.dropout(self.output(conc))\n",
    "        if label is not None:\n",
    "            loss_fct = nn.BCEWithLogitsLoss()\n",
    "            loss = loss_fct(out.view(-1,self.num_classes).float(), label.view(-1,self.num_classes).float())\n",
    "            return loss\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_mlogloss(label,pred):\n",
    "    score = 0\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(17):\n",
    "            if pred[i][j] == 0:\n",
    "                pred[i][j] +=1e-10\n",
    "            elif pred[i][j] == 1:\n",
    "                pred[i][j] -=1e-10\n",
    "            score += label[i][j]*np.log(pred[i][j])+(1-label[i][j])*np.log(1-pred[i][j])\n",
    "    score /= (len(pred)*17*(-1))\n",
    "    return 1-score\n",
    "\n",
    "def validation_funtion(model, valid_dataloader, valid_torchdata, mode='valid'):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    labels_list = []\n",
    "    if mode == 'valid':\n",
    "        for i, (description, label) in enumerate(tqdm(valid_dataloader)):\n",
    "            output = model(description.to(DEVICE))\n",
    "            pred_list += output.sigmoid().detach().cpu().numpy().tolist()\n",
    "            labels_list += label.detach().cpu().numpy().tolist()\n",
    "        auc = roc_auc_score(labels_list,pred_list, multi_class='ovo')\n",
    "        logloss = log_loss(labels_list, pred_list)\n",
    "        mlogloss = metric_mlogloss(labels_list, pred_list)\n",
    "        return mlogloss, auc, logloss\n",
    "    else:\n",
    "        for i, (description) in enumerate(tqdm(valid_dataloader)):\n",
    "            output = model(description.to(DEVICE))\n",
    "            pred_list += output.sigmoid().detach().cpu().numpy().tolist()\n",
    "        return pred_list\n",
    "    \n",
    "                            \n",
    "def train(model, train_dataloader, valid_dataloader, valid_torchdata, epochs, early_stop=None):\n",
    "    global logger\n",
    "#     ema = EMA(model, 0.999)\n",
    "#     ema.register()\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    embed_pa = ['embedding.weight']\n",
    "    optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in embed_pa)]},\n",
    "                                    {'params': model.embedding.parameters(), 'lr': 5e-5}]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=1e-3, amsgrad=True, weight_decay=5e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=3, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "#     scheduler = CyclicLR(optimizer, base_lr=1e-3, max_lr=3e-3,\n",
    "#                step_size=30, mode='exp_range',\n",
    "#                gamma=0.99994)\n",
    "#     opt = SWA(optimizer, swa_start=100, swa_freq=5, swa_lr=1e-4)\n",
    "    total_loss = []\n",
    "    train_loss = []\n",
    "    best_mlogloss = -np.inf\n",
    "    best_auc = -np.inf\n",
    "    best_loss = np.inf\n",
    "    no_improve = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "#         fgm = FGM(model)\n",
    "        bar = tqdm(train_dataloader)\n",
    "        for i, (description, label) in enumerate(bar):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(description.to(DEVICE), label.to(DEVICE))\n",
    "            loss = output\n",
    "            loss.backward()\n",
    "            train_loss.append(loss.item())\n",
    "            \n",
    "#             fgm.attack()\n",
    "#             loss_adv = model(describe.to(DEVICE), label.to(DEVICE))\n",
    "#             loss_ad = loss_adv\n",
    "#             loss_ad.backward()\n",
    "#             fgm.restore()\n",
    "            \n",
    "            scheduler.step(epochs + i / len(train_dataloader))\n",
    "#             scheduler.batch_step()\n",
    "            optimizer.step()\n",
    "#             ema.update()\n",
    "            bar.set_postfix(tloss=np.array(train_loss).mean())\n",
    "#         opt.swap_swa_sgd()\n",
    "#         ema.apply_shadow()\n",
    "        mlogloss, auc, logloss = validation_funtion(model, valid_dataloader, valid_torchdata, 'valid')\n",
    "#         ema.restore()\n",
    "        print('train_loss: {:.5f}, mlogloss: {:.5f}, auc: {:.5f}, log_loss: {:.5f}\\n'.format(train_loss[-1],mlogloss,auc,logloss))\n",
    "        logger.info('Epoch:[{}]\\t mlogloss={:.5f}\\t auc={:.5f}\\t log_loss={:.5f}\\t'.format(epoch,mlogloss,auc,logloss))\n",
    "        global model_num\n",
    "        if early_stop:\n",
    "            if mlogloss > best_mlogloss:\n",
    "                best_mlogloss = mlogloss\n",
    "                best_auc = auc\n",
    "                best_loss = train_loss[-1]\n",
    "#                 ema.apply_shadow()\n",
    "                torch.save(model.state_dict(), '{}_model_{}.bin'.format(NAME, model_num))\n",
    "#                 ema.restore()\n",
    "            else:\n",
    "                no_improve += 1\n",
    "            if no_improve == early_stop:\n",
    "                model_num += 1\n",
    "                break\n",
    "            if epoch == epochs-1:\n",
    "                model_num += 1\n",
    "        else:\n",
    "            if epoch >= epochs-1:\n",
    "                torch.save(model.state_dict(), '{}_model_{}.bin'.format(NAME, model_num))\n",
    "                model_num += 1\n",
    "    return best_mlogloss, best_auc, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "    logger.removeHandler(sh)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD = 10\n",
    "kf = StratifiedKFold(n_splits=FOLD, shuffle=True, random_state=SEED)\n",
    "model_num = 1\n",
    "test_preds_total = collections.defaultdict(list)\n",
    "logger = get_logger('{}.log'.format(NAME))\n",
    "best_mlogloss = []\n",
    "best_auc = []\n",
    "best_loss = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(np.arange(train_data.shape[0]), train_data.label.values)):\n",
    "    print(str(i+1), '-'*50)\n",
    "    tra = [train_dataset[index] for index in train_index]\n",
    "    val = [train_dataset[index] for index in test_index]\n",
    "    print(len(tra))\n",
    "    print(len(val))\n",
    "    train_dataloader, train_torchdata = get_dataloader(tra, mode='train')\n",
    "    valid_dataloader, valid_torchdata = get_dataloader(val, mode='valid')\n",
    "    model = NeuralNet(w2v_model.wv.vectors.shape[0]+1,w2v_model.wv.vectors.shape[1]+fasttext_model.wv.vectors.shape[1],embeddings=True)\n",
    "    model.to(DEVICE)\n",
    "    mlogloss,auc,loss = train(model,train_dataloader,\n",
    "                    valid_dataloader,\n",
    "                    valid_torchdata,\n",
    "                    epochs=100,\n",
    "                    early_stop=5)\n",
    "    torch.cuda.empty_cache()\n",
    "    best_mlogloss.append(mlogloss)\n",
    "    best_auc.append(auc)\n",
    "    best_loss.append(loss)\n",
    "for i in range(FOLD):\n",
    "    print('- 第{}折中，best mlogloss: {}   best auc: {}   best loss: {}'.format(i+1, best_mlogloss[i], best_auc[i], best_loss[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 第1折中，best mlogloss: 0.9754145609566004   best auc: 0.9989513662197707   best loss: 0.370415061712265\n",
    "- 第2折中，best mlogloss: 0.9775772409438741   best auc: 0.9973311857745906   best loss: 0.3379441797733307\n",
    "- 第3折中，best mlogloss: 0.9772133082300454   best auc: 0.9960571601879297   best loss: 0.3513079881668091\n",
    "- 第4折中，best mlogloss: 0.9771865291289459   best auc: 0.9979561512414352   best loss: 0.3584481477737427\n",
    "- 第5折中，best mlogloss: 0.9822103989262112   best auc: 0.9976901364500118   best loss: 0.31363099813461304\n",
    "- 第6折中，best mlogloss: 0.9818852323759074   best auc: 0.9982848531202542   best loss: 0.3530382215976715\n",
    "- 第7折中，best mlogloss: 0.9791612508478834   best auc: 0.9988892924512657   best loss: 0.33308154344558716\n",
    "- 第8折中，best mlogloss: 0.98057134271314   best auc: 0.9992422836399381   best loss: 0.33398792147636414\n",
    "- 第9折中，best mlogloss: 0.9789818534174326   best auc: 0.9985592272795051   best loss: 0.3301367461681366\n",
    "- 第10折中，best mlogloss: 0.980113297001539   best auc: 0.9970231641648527   best loss: 0.3556895852088928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_num = 11\n",
    "model = NeuralNet(w2v_model.wv.vectors.shape[0]+1,w2v_model.wv.vectors.shape[1]+fasttext_model.wv.vectors.shape[1],embeddings=True)\n",
    "model.to(DEVICE)\n",
    "test_preds_total = []\n",
    "test_dataloader, test_torchdata = get_dataloader(test_dataset, mode='test')\n",
    "for i in range(1,model_num):\n",
    "    model.load_state_dict(torch.load('{}_model_{}.bin'.format(NAME, i)))\n",
    "    test_pred_results = validation_funtion(model, test_dataloader, test_torchdata, 'test')\n",
    "    test_preds_total.append(test_pred_results)\n",
    "test_preds_merge = np.sum(test_preds_total, axis=0) / (model_num-1)\n",
    "pres_fold = [[str(p) for p in li] for li in test_preds_merge]\n",
    "pres_all = [' '.join(p) for p in pres_fold]\n",
    "str_w = ''\n",
    "sub_id = test_data['report_ID'].values\n",
    "with open('submit.csv','w') as f:\n",
    "    for i in range(len(sub_id)):\n",
    "        str_w += sub_id[i] + '|,' + '|' + pres_all[i] + '\\n'\n",
    "    str_w = str_w.strip('\\n')\n",
    "    f.write(str_w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
